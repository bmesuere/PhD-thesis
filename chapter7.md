# The future of Unipept {.chapter data-running-title='The future of Unipept'}

1-2 paragraph introduction

## From metagenomics to metaproteomics (and back again) {data-running-title='From metagenomics to metaproteomics (and back again)'}
* WP1 (FWO)
* preliminary results ITG

Consider the problem of taxonomically assigning a DNA read resulting from a shotgun metagenomics experiment. The initial step in this process, shared by all current metagenomics analysis tools, is to perform inexact matching (e.g. Blast (Altschul et al., 1990), BLAT (Kent, 2002), USearch (Edgar, 2010)) of the DNA read against a reference database and to retain only those hits that are considered sufficiently similar. Some tools only consider the best match and assign the DNA read based on the taxonomic annotation of that match in the reference database (Seshadri et al. 2007). Other tools consider multiple matches (e.g. a fixed number of best matches or all matches that meet certain cutoff criteria) and use an aggregation strategy to reduce the taxonomic annotation of those matches in the reference database into a consensus taxonomic assignment of the DNA read (Huson et al., 2007; Meyer et al., 2008; Luo et al., 2014). Pep2Pro (Askenazi et al., 2010), for example, only makes a taxonomic assignment if all retained matches share the same taxonomic annotation in the reference database. This is particularly bad since it strongly depends on the sampling bias of the reference database and favors dominant taxa. A far better approach that is generally recommended in review papers, is to compute the lowest common ancestor (LCA) of all retained matches (Bazinet & Cummings, 2012; Mande et al., 2012; Michael 2013). However, such a strategy should be implemented carefully, as the NCBI Taxonomy for example contains a large number of “false” taxa (strain-level assignments; Federhen et al. 2014) that should be discarded before applying the LCA procedure, and the procedure should also be robust against “false” taxonomic annotations in the reference databases. To avoid “false” reference sequences, databases are often compiled from a list of whole-genome sequences, reducing the taxonomic identification space to only those taxa whose complete genome was sequenced, or from a list of quality controlled 16S rRNA sequences, ignoring the majority of DNA reads that do not contain 16S rRNA fragments and reducing the taxonomic identification space to the prokaryotes (Hunter et al., 2013).

We propose to extend the Unipept platform with a shotgun metagenomics analysis pipeline that solves the taxonomic assignment of DNA reads following a radically different approach. Because Unipept was originally designed for shotgun metaproteomics analysis, we will translate the metagenomics problem into a series of metaproteomics problems, solve the latter using existing Unipept functionality, and map the results back into the metagenomics context. Figure [C] outlines how this approach can be implemented. Instead of directly matching a DNA read against the sequences in a reference database, we start by predicting protein coding genes on the DNA read. This can be done using software packages like FragGeneScan (Rho et al., 2010), which is particularly interesting since it makes use of a hidden Markov model that allows to i) find genes that are only partially covered in the DNA read, ii) correct read errors induced by current NGS technologies and iii) predict the translation table that needs to be used to translate the nucleotide sequence into a protein sequence. Assembly of partial peptide fragments into more complete protein sequences using the LCA short peptide assembler (Yang et al., 2013) is an optional post-processing step, which is in contrast with more traditional approaches that start with assembly of DNA reads, followed by gene prediction (Nielsen et al., 2014). This approach is far more accurate than using nucleotide assembly-based strategies followed by gene prediction. Following this strategy, we essentially transform the metagenomics problem into a metaproteomics problem that can already be solved by Unipept. The predicted (partial) protein sequences are digested in silico into a series of tryptic peptides that are individually identified taxonomically using the Unipept LCA procedure (Figure [A]; Figure [C], 3a). Unipept uses exact matching to map peptides to proteins in UniProt, which is not only faster than the inexact matching procedures that are used traditionally but also avoids the need for cutoff criteria: the LCA is computed based on all exact matches. Although it might seem that the Unipept approach will be less accurate because inexact (DNA) matching is less stringent than exact (protein) matching, Tanca et al. (2013) have shown in a controlled experiment that Unipept has three to five times more correct identifications compared to MEGAN (Huson et al., 2007; which also implements an LCA approach but based on Blast matches), with more than half the number of incorrect identifications. In addition, since Unipept precomputes the LCA for all tryptic peptides extracted from the UniProt database, this step in the identification process is extremely fast.

Figure [C]: General outline of the possible strategies for taxonomic identification of a DNA read by directly predicting gene fragments on the DNA read, breaking up the partial proteins into tryptic peptides and finding all proteins having exact matches with the tryptic peptides. In a next step, a consensus taxonomic assignment can be computed from (1) the proteins having multiple peptide hits, (2) the taxa having multiple peptide hits, or (3) the taxonomic assignments for the individual peptides.

Although the taxonomic identification process could stop at this point by simply pooling all individual peptide-based identifications for all DNA reads in a shotgun metagenomics data set, this approach would ignore an important piece of information: all tryptic peptides derived from the same DNA read come from the same organism. This is where metagenomics has an advantage over metaproteomics, because we can perform an additional aggregation step that bundles the individual peptide-based identifications into a single consensus identification for the DNA read (Figure [C], 3b), which can be more accurate (e.g. less specific identifications are overruled by more specific identifications) and more robust against incorrect identifications (e.g. less supported identifications are pruned). Such an aggregation essentially transforms the metaproteomics identifications back into a consensus metagenomics identification. The individual peptide-based identifications could for example be aggregated by again computing their LCA (note that these LCA computations cannot be precomputed). Wood and Salzberg (2014) propose another aggregation technique based on determining the maximal root-to-leaf (RTL) path of all individual identifications in the taxonomic tree.

Both of the above approaches are based on the idea that aggregation at this step should reduce a series of individual taxonomic assignments into a single taxonomic assignment. However, it is statistically more sensitive to postpone such a crisp reduction until later stages (where the overall biodiversity distribution of the whole sample is analyzed and visualized) and to apply more fuzzy reductions at this stage that represent the aggregated biodiversity as a weighed hierarchical distribution. One possible approach is illustrated in Figure [D]. Instead of reducing the weighed hierarchical distribution formed by the individual taxonomic assignments to a single taxonomic node (for example by taking the lowest node with weight 1 in the LCA approach, or selecting the leaf node with the highest weight in the RTL approach), we may represent the taxonomic assignment of a single DNA read (or protein) as a weighed tree, and aggregate the weighed trees for all DNA reads sequenced from the sample by adding the weights of the corresponding nodes in the hierarchy and divide these weights by the number of hierarchies merged (i.e. normalization over all proteins or DNA reads). This essentially computes the root-to-node (RTN) path for every node in the tree (in contrast to computing the path only for the leafs with the RTL approach). As an end result, the biodiversity distribution of a particular sample is represented as a weighed tree instead of a count table (or tree), which is just a generalization that can equally well be analyzed and visualized. A proof-of-concept of this workflow has been implemented in collaboration with the EBI Metagenomics team, showing very accurate identifications that are not restricted to the prokaryotes as with most of the other metagenomics pipelines. This basic implementation already sequentially processes 12 million reads in 8 hours.

Starting from a DNA read that is broken into a series of tryptic peptides, another aggregation approach is to map each peptide onto its matching proteins (Figure [C], 1a) and aggregate the taxonomic annotation only from those proteins that match all or a minimal number of the peptides from the query protein (Figure [C], 1b), e.g. using the Unipept LCA approach. A variant thereof is to map each peptide onto the taxonomic annotations of all its matching proteins (Figure [C], 2a) and only aggregate the taxonomic annotations that were matches for all or a minimal number of the peptides from the query protein or DNA read (Figure [C], 2b), e.g. using the Unipept LCA approach. This will probably lead to more accurate results compared to the previous approaches, but at the cost of a much longer computing time because some universal peptides will be found in large numbers of proteins and/or taxa.

Apart from in silico splitting a (partial) protein into (non-overlapping) tryptic peptides, the protein may also be split into (overlapping) k-mers. This approach has been introduced in KRAKEN (Wood and Salzberg, 2014). The same aggregation strategies that were discussed for peptide-based identifications can also be applied onto the k-mers. Note however that KRAKEN introduces bias by not correcting for identifications derived from overlapping k-mers, which can be easily remedied by normalizing the identifications per protein residue. It is expected that identification based on overlapping k-mers will increase accuracy compared to peptide-based identification (which is only motivated in a metagenomics context by the fact that Unipept already has fast peptide-based indexes for usage in a metaproteomics context), but at the cost of a higher memory footprint to store the index and a performance penalty because the number of overlapping k-mers is higher than the number of non-overlapping tryptic peptides.

This work package aims at extending Unipept with multiple strategies for the taxonomic identification of DNA reads from shotgun metagenomics data sets (at least the existing and novel strategies discussed above), expanding the scope of the platform from metaproteomics to metagenomics. A benchmark study will be performed to compare the speed of execution, storage requirements and accuracy of the different strategies (including the traditional identification approaches implemented in other metagenomics tools), using real and simulated shotgun metagenome data. Our target is that Unipept should be able to accurately identify at least 1 million 500bp DNA reads per minute, where other packages take hours to perform the same computations. This would enable us, in collaboration with the EBI Metagenomics team, to re-analyze all metagenomics samples from the EBI Metagenomics Archive at regular time interval (e.g. monthly). At present, each data set submitted to this archive is analyzed only once at the time of submission, where reference databases grow at an exponential rate leading to less undersampled biodiversity.


Figure [D]: Weighed hierarchical distribution algorithm: 1) assign (normalized) weights to taxa based on number of individual (peptide) identifications, 2) take into account the hierarchical structure by distributing the weight of a parent node among the identified leaf nodes of its subtree (optional); distribution of parent weights may or may not take into account weights already assigned to leafs, 3) normalize weights across the entire tree (each protein identification is assigned the same weight), 4) take into account the hierarchical structure by computing parent weights as the sum of their child weights (optional).

Further proof-of-concept of the Unipept Shotgun Metagenomics Pipeline will be performed on metagenomics data generated from faecal samples received from the UZ Leuven Pediatric Clinic. These samples were collected to analyze gut microbial dysbiosis in CF patients. Samples are obtained from 19 patients and at least one unaffected sibling at 8 time points over a 2-year period. Previous studies on these samples using DGGE (Duytschaever et al., 2011; Duytschaever et al., 2013) revealed a severe dysbiosis and temporal instability in CF patients. This was confirmed by metaproteomics on a subset of these samples, which additionally provided evidence of intestinal inflammation (Debyser et al., 2015). Additional shotgun metagenomics and metaproteomics experiments on the same samples generated within the current project will allow us to perform a more in-depth study of the observed shifts in the gut microbiota of CF patients. We also received additional faecal samples that allow to investigate the effect of Ivacaftor, a potentiator of CFTR containing the G551D mutation. These samples were collected from 8 CF patients at different time points before and after start of the therapy. The aim is to analyze whether restoring CFTR activity also corrects the gut microbiota dysbiosis in CF patients, based on metagenomics and metaproteomics data sets generated within the current project.

Current NGS technologies allow metagenomes to be sequenced much deeper than metaproteomes, which will provide us with a more detailed insight into the complex biodiversity of the samples. Combined metagenomes and metaproteomes of the same sample will also allow us to link the functional potential encoded in genomes to expression levels measured in the sample, and to associate differential functional expressions with shifts in the biodiversity. Moreover, to obtain optimal identification of the peptides resulting from metaproteomics experiments, most studies currently match peptides against shotgun metagenomics data sets from the same samples (Erickson et al., 2012; Kolmeder et al., 2014). We will investigate if this is still a requirement in the context of analyzing human faecal samples, now that public databases may cover most of the protein diversity found in faecal samples after they have been enriched with proteins from large-scale sequencing efforts (HMP Consortium, 2012; Li et al., 2014). We postulate that integrated catalogs of reference genes in the human gut microbiome are sufficient to identify most peptides from shotgun metaproteomics experiments. To prove this statement, we will compare the identification rate and depth of taxonomic analysis by searching the shotgun metaproteomics data we obtained from this and a previous study against the matched metagenomes generated in the current project, with analysis results obtained using UniProt and/or the IGC (Li et al. 2014) as reference databases.


## Functional analysis of metagenomics/metaproteomics data
At the heart of Unipept lies an index structure for fast mapping of tryptic peptides onto all UniProt protein sequences having exact substring matches (Figure [A]). The taxonomic and functional annotations on matched UniProt records can be used to infer taxonomic and functional assignments for the individual peptides from metaproteomics experiments using the Tryptic Peptide Analysis feature of Unipept (http://unipept.ugent.be/search/single). The Shotgun Metaproteomics Analysis Pipeline (http://unipept.ugent.be/datasets) of Unipept currently supports streamlined identification, analysis and visualization of all peptides from a metaproteomics experiment, and WP1 discusses how this pipeline can be extended for biodiversity analysis of shotgun metagenomics experiments. The goal of this work package is to extend both Unipept pipelines with a statistical data analysis and visualization framework for functional analysis of metagenomics and metaproteomics experiments that goes beyond the toolset that is currently provided by metaproteomics/metagenomics tools both in terms of performance, accuracy and interactivity.

The traditional approach for functional analysis of shotgun metagenomics/metaproteomics data sets provided by current tools is to represent functional ontology annotations (e.g. Gene Ontology, GO; Ashburner et al., 2000) as charts that are often zoomable per ontological level, or to map Enzyme Commission (EC) numbers onto metabolic pathways such as those provided by the Kyoto Encyclopedia of Genes and Genomes (KEGG; Kanehisa & Goto, 2000). The implementation of this kind of functional and metabolic pathway analysis tools in Unipept will be quite straightforward, as fast peptide-to-protein matching is already part of the Unipept kernel, functional assignments of all UniProt records are already parsed in the Unipept database, and Unipept already has implemented flexible and interactive visualizations that can be reused for basic functional analysis. However, although many metagenomics and metaproteomics papers base their discussion of the functional complement of environmental samples on GO pie charts and/or metabolic pathway expression levels, we dare to claim that such visualizations usually do not provide deep biological insight. We therefore see an implementation of the basis functional analysis tools only as a stepping stone for the functional analysis framework of Unipept, that can both be made more accurate by taking advantage of aggregation strategies as outlined in WP1 (discussed in the next paragraph) and can lead to in-depth comparative functional analysis (discussed in WP3).

The aggregation strategies for taxonomic assignments as discussed in WP1 cannot directly be applied for the aggregation of multiple GO assignments (e.g. taken from all proteins matching a peptide sequence, or taken from all individual peptides in a (partial) protein predicted on a DNA read), as GO is structured as a directed acyclic graph (DAG) rather than a tree (hierarchy) and a single reference protein may have multiple GO annotations in addition to a single taxonomic assignment. However, the weighed hierarchical distribution algorithm discussed in WP1 (of which the LCA and maximal RTL algorithms are special cases) can be extended into a weighed DAG distribution algorithm (Figure [E]). In this case, top-down redistribution of the weight of a parent node (step 2 in the hierarchical algorithm) might have multiple contributions to the same leaf node as a consequence of multiple parallel node-to-leaf paths. Bottom-up progression of leaf weights (step 4 in the hierarchical algorithm) might have to split a child weight over multiple parent nodes. This is a generalization of the aggregation strategy implemented in the commercial software package Blast2GO (Conese et al., 2005), from which we can use the idea to incorporate Evidence Code weights to promote the assignment of functional annotations with experimental evidence and penalize electronic annotations or low traceability. The latter is important, since it has been predicted that 13-15% of the functional annotations in reference databases contain database propagation errors (Brenner, 1999). Similar to WP1, we can avoid using Blast by relying on the Unipept index. As with the LCAs of taxonomic assignments, aggregation of functional assignments of the proteins matching a tryptic peptide can be precomputed for all peptides extracted from UniProt and cached in the Unipept database to increase performance of downstream analysis. One of the milestones of this work package is to come up with fast implementations of functional aggregation strategies for metaproteomics and metagenomics data sets, and to benchmark the performance and accuracy of the different alternatives along the lines of the taxonomic benchmark outlined in WP1.

The Unipept extensions from this work package will be applied for the functional and metabolic pathway analysis of the metagenomics and metagenomics data sets generated from the faecal samples of CF patients and their healthy siblings (see WP1). The most pronounced observation in diseases associated with gut dysbiosis (inflammatory bowel disease, colon cancer) is a substantial decrease of “health promoting” species such as Faecalibacterium prausnitzii. This was also observed in our cross sectional shotgun metaproteomics analysis of faecal samples from CF patients (Debyser et al., in preparation). These organisms are typical producers of butyrate, a key metabolite used as energy source by colonocytes that has anti-inflammatory properties. It therefore seems like a logical conclusion that dysbiosis is associated with a lack of butyrate production. However, systematic analysis has shown recently that more bacteria have butyrate-producing pathways than was previously anticipated, and that these organisms show high abundance (up to 40%) in stool samples of healthy individuals investigated by the HMP consortium (Vital et al., 2014). Causal inferences between depletion of certain taxa and observed functional shifts should therefore be drawn carefully. As metaproteomics provides direct abundance measurements of the enzymes involved in metabolic pathways, this gives a better reflection of the potential changes in biochemical pathways. The functional extensions of Unipept will therefore be applied for the analysis of shotgun metaproteomics data sets that were previously generated from the faecal samples of CF patients and their unaffected siblings. We will specifically address the question whether dysbiosis indeed results in a loss of enzymes associated with butyrate production. Other interesting questions regarding functional changes due to dysbiosis in CF patients include a potential raise in expression of antibiotic resistance genes, mucin degrading extracellullar glycosidases, and increased abundance of proteins involved in inflammatory pathways at level of host proteins.

Figure [E]: Example of applying the weighed DAG distribution algorithm, showing the initial frequencies of GO term annotations of all UniProt proteins that match the tryptic peptides SSWWAHVEMGPPDPILGVTEAYK or DTNSK (left) and the result after redistributing the weights top-down and bottom-up (right).

## Comparative analysis of metagenomics/metaproteomics data
* WP3 (FWO)
* challenges in visualisations

Recent review papers about the current state-of-the-art in metaproteomics praise Unipept for the performance and accuracy of its taxonomic identification pipeline, and also for its interactive visualization framework that helps to explore the biodiversity in complex environmental samples (Seifert et al., 2013; Kolmeder & de Vos, 2014). However, most environmental studies do not merely apply metagenomics and metaproteomics to gain insight in the taxonomic, functional and metabolic composition of individual samples, but rather want to investigate observed compositional shifts between multiple samples. In general, there are two types of environmental studies. Cross-sectional studies investigate samples collected from distinct environmental niches at one specific point in time or under a time-independence assumption in order to observe causal effects of one or more environmental factors upon sample composition. Longitudinal studies involve repeated observations of the same environmental niche over a period of time to study shifts in sample composition that may be correlated to certain temporal events.

The goal of this work package is to develop an open source, standalone, modular and extensible web framework for statistical analysis and data visualization of multiple metagenomics or metaproteomics data sets from cross-sectional and longitudinal studies (Mehlan et al., 2013; Wang et al., 2015). As showcased by Unipept, such a statistical analysis and data visualization toolbox nowadays can be implemented as a highly responsive client application hosted in a web browser using the latest web technologies. In particular we will make use of the latest HTML5 technologies (an official standard since October 2014) including Web Workers for parallel processing and Local Storage for browser caching, as well as high-performance JavaScript libraries including D3.js for interactive data manipulation and visualization. The design goal to make this a modular framework will allow third-party web developers to reuse individual components in their own web applications. The extensibility of the framework will allow software developers to plug new statistical or visualization components into the framework by reusing underlying components. This will allow the statistical analysis and data visualization framework to be used as a standalone client application that enables users to upload multi-sample contingency tables that were processed by metagenomics or metaproteomics pipelines (e.g. in the Biological Observation Matrix (BIOM) format (McDonald, 2012) which is an interoperable format supported by all major pipelines) or to embed the framework or some of its components in other web applications. Of course, the extensions of Unipept for comparative analysis will be based on the newly developed framework for statistical analysis and data visualization.

As an initial step, the pluggable visualization framework will be populated with basic interactive visualizations that are readily available from the JavaScript D3 gallery (tables, pie charts, (stacked) bar charts, heat maps)  and existing Unipept visualizations (sunburst, treemap, treeview). This will be complemented with in-browser implementations of basic statistical tools (principal component analysis, multidimensional scaling, hierarchical clustering) and more elaborate statistical and visualization approaches for comparative analysis such as Lefse (Segata et al., 2011) for differential taxonomic and functional analysis and UniFrac (Lozupone & Knight, 2005) to calculate distances between organismal communities using phylogenetic information. In contrast to the existing MetaSee framework (Song et al., 2012), however, we aim at implementing our framework according to the Model-View-Controller (MVC) design pattern. This will allow to store the data only once (model) as a reduction of the memory footprint for the browser, with the possibility of interactive operations (controllers) to navigate the data that impact how multiple simultaneous visualizations (views) of the same data are rendered. As such, additional components can be plugged into the framework as independent controllers or viewers. This will require the definition of a standard data representation layer (tabular, tree or (directed acyclic) graph data structures) and a standard data manipulation interface. For example, many metagenomics tools visualize the biodiversity in a sample not as a tree (which may include simply too much information), but sliced at a specific taxonomic rank (e.g. as a pie chart at the phylum level). From the MVC perspective, this slice operation is not implemented as a static conversion from a tree format to a tabular format but as a dynamic view of the tree as a table, which allows more dynamic visualizations that navigate top-down and bottom-up in the tree. Moreover, the same controller can be applied to all sample data sets at once, allowing interactive comparative visualizations.

The statistical framework for comparative analysis of metagenomic and metaproteomics data sets will also be extended with the development and improvement of methods that build upon the Zero-Inflated Negative Binomial (ZINB) Mixed Model framework, so as to accommodate for zero excess and correlation in longitudinal designs. Initially, our efforts will be directed towards differential abundance estimation at the level of a single taxonomic unit (OTU) and towards algorithms that can borrow strength across OTUs for improving the estimation of the ZINB parameters on each TU. In a second stage, our methods will be extended to account for correlation and finally, we will incorporate additional hierarchical levels imposed by the taxonomic and/or functional annotation. As soon as the methods are stable and have proven their value for the analysis of the samples of the CF patients and their healthy siblings, the methods will be implemented as components of the web-based analysis and visualization framework described above.

During an internship with the UniProt team at the European Bioinformatics Institute (Hinxton-Cambridge, UK; FWO travel grant K219113N), an initial prototype of a comparative functional analysis tool has been implemented (Figure [F]). In its current implementation, aggregated functional assignments of the tryptic peptides of one or two samples are mapped onto KEGG pathways, and the pathways are preferentially ranked based on the number of differentially expressed enzymatic reactions. The prototype requires a number of extension before it can be deployed in an official Unipept release. First of all, as discussed in WP2, the functional assignments could equally well be based on the DNA reads from metagenomics samples. The prototype can also be extended to support cross-sectional studies that want to compare the functional shifts between multiple samples from two distinct environmental niches. Instead of the current ranking heuristic, we also plan to adopt the statistical framework for differentially expressed proteins as implemented in MetaStats (White et al., 2009) and its extension MetaPath (Liu & Pop, 2010; Liu & Pop 2011), which allows much more granular detection of subpathways that are globally over- or underexpressed between subpopulations. The current third-party implementation of these tools is quite slow, but it seems that a fast re-implementation is well within reach of the Computational Biology group.


Figure [F]: Prototype for comparative functional analysis in Unipept.

Another goal of this work package is to add components to the framework for statistical analysis and data visualization that allow integrated taxonomic and functional analysis. Current metagenomics and metaproteomics tools implement taxonomic and functional analysis merely as separate pipelines, whereas biological comparison between the samples would greatly benefit from statistical tools and visualization aids that intertwine both viewpoints. As taxonomic and functional assignments are made for the individual peptides (metaproteomics) or DNA reads (metagenomics), they establish a complex graph (network) where both pieces of the puzzle come together. However, tools for visualizing metabolic pathways as KEGG or iPath (Yamada et al., 2011) do not provide additional annotations to highlight "who is doing what" in the environment. Having integrated visualizations and graph-based statistical tools would allow to search for functional changes beyond what can be explained by a mere shift in populations, and vice versa, to search for shifts in populations that do not result in functional changes.

All comparative analysis tools developed within this work package will be applied in our longitudinal and Ivacaftor studies of the gut microbiota of CF patients, to answer the specific question whether there is a return to a more "healthy" microbiota in CF patients that received Ivacaftor treatment. The quantitative metaproteomics analysis will be benchmarked against the commercially available Scaffold software (http://www.proteomesoftware.com/products/scaffold), which is in our view one of the best tools currently available to analyze shotgun proteomics data. However, this tool is not specifically designed for usage in the context of metaproteomics.

## Other challenges?
* stand alone visualisations?
* docker?
